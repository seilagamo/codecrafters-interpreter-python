"""Parser"""

import sys

from gen import expr, stmt

from . import scanner
from .astprinter import AstPrinter
from .tokens import Token, TokenType


class Parser[T]:
    """
    A parser class that processes a list of tokens and constructs an abstract
    syntax tree (AST) for expressions.

    The parser class takes a list of tokens generated by a lexer and processes
    them according to the rules of the language's grammar. It contains various
    methods for parsing different levels of expressions, handling errors, and
    synchronizing after error recovery.
    """

    class ParseError(Exception):
        """A custom exception for parsing errors."""

    def __init__(self, tokens: list[Token]):
        self._tokens = tokens
        self._current = 0
        self.parse_errors: list[str] = []

    def parse(self) -> list[stmt.Stmt[T]]:
        """parse"""
        statements: list[stmt.Stmt[T]] = []
        try:
            while not self._is_at_end():
                statements.append(self._statement())
            return statements

        except self.ParseError:
            return statements

    def expression(self) -> expr.Expr[T]:
        """Parse an expression."""
        return self._equality()

    def _equality(self) -> expr.Expr[T]:
        _expr = self._comparison()

        while self._match(TokenType.BANG_EQUAL, TokenType.EQUAL_EQUAL):
            operator = self._previous()
            right = self._comparison()
            _expr = expr.BinaryExpr(_expr, operator, right)

        return _expr

    def _match(self, *types: TokenType) -> bool:
        for _type in types:
            if self._check(_type):
                self._advance()
                return True
        return False

    def _check(self, _type: TokenType) -> bool:
        if self._is_at_end():
            return False
        return self._peek().type == _type

    def _advance(self) -> Token:
        if not self._is_at_end():
            self._current += 1
        return self._previous()

    def _is_at_end(self) -> bool:
        return self._peek().type == TokenType.EOF

    def _peek(self) -> Token:
        return self._tokens[self._current]

    def _previous(self) -> Token:
        return self._tokens[self._current - 1]

    def _comparison(self) -> expr.Expr[T]:
        _expr = self._term()

        while self._match(
            TokenType.GREATER,
            TokenType.GREATER_EQUAL,
            TokenType.LESS,
            TokenType.LESS_EQUAL,
        ):
            operator = self._previous()
            right = self._term()
            _expr = expr.BinaryExpr(_expr, operator, right)

        return _expr

    def _term(self) -> expr.Expr[T]:
        _expr = self._factor()

        while self._match(TokenType.MINUS, TokenType.PLUS):
            operator = self._previous()
            right = self._factor()
            _expr = expr.BinaryExpr(_expr, operator, right)

        return _expr

    def _factor(self) -> expr.Expr[T]:
        _expr = self._unary()

        while self._match(TokenType.SLASH, TokenType.STAR):
            operator = self._previous()
            right = self._unary()
            _expr = expr.BinaryExpr(_expr, operator, right)

        return _expr

    def _unary(self) -> expr.Expr[T]:
        if self._match(TokenType.BANG, TokenType.MINUS):
            operator = self._previous()
            right = self._unary()
            return expr.UnaryExpr(operator, right)

        return self._primary()

    def _primary(self) -> expr.Expr[T]:
        if self._match(TokenType.FALSE):
            return expr.LiteralExpr(False)
        if self._match(TokenType.TRUE):
            return expr.LiteralExpr(True)
        if self._match(TokenType.NIL):
            return expr.LiteralExpr(None)

        if self._match(TokenType.NUMBER, TokenType.STRING):
            return expr.LiteralExpr(self._previous().literal)

        if self._match(TokenType.LEFT_PAREN):
            _expr = self.expression()
            self._consume(TokenType.RIGHT_PAREN, "Expect ')' after expression.")
            return expr.GroupingExpr(_expr)

        raise self._error(self._peek(), "Expect expression.")

    def _consume(self, _type: TokenType, message: str) -> Token:
        if self._check(_type):
            return self._advance()
        raise self._error(self._peek(), message)

    def _error(self, token: Token, message: str) -> ParseError:
        self._report_error(token, message)
        return self.ParseError()

    def _report_error(self, token: Token, message: str) -> None:
        if token.type == TokenType.EOF:
            self.parse_errors.append(f"[line {token.line}] Error: {message}")
        else:
            self.parse_errors.append(
                f"[line {token.line}] Error at '{token.lexeme}': {message}"
            )

    def _synchronize(self) -> None:
        self._advance()

        while not self._is_at_end():
            if self._previous().type == TokenType.SEMICOLON:
                return

            match self._peek().type:
                case (
                    TokenType.CLASS
                    | TokenType.FUN
                    | TokenType.VAR
                    | TokenType.FOR
                    | TokenType.IF
                    | TokenType.WHILE
                    | TokenType.PRINT
                    | TokenType.RETURN
                ):
                    return

            self._advance()

    def _statement(self) -> stmt.Stmt[T]:
        if self._match(TokenType.PRINT):
            return self._print_statement()

        return self._expression_statement()

    def _print_statement(self) -> stmt.Stmt[T]:
        value = self.expression()
        self._consume(TokenType.SEMICOLON, "Expect ';' after value.")
        return stmt.PrintStmt(value)

    def _expression_statement(self) -> stmt.Stmt[T]:
        _expr = self.expression()
        self._consume(TokenType.SEMICOLON, "Expect ';' after expression.")
        return stmt.ExpressionStmt(_expr)


def parse_cmd(content: str) -> None:
    """The `parse` method processes the given contents of a source file by
    scanning it into tokens, parsing those tokens into an abstract syntax
    tree (AST), and then printing the resulting AST."""
    expression, parse_errors = parse_expression(content)

    if parse_errors:
        _print_parse_errors(parse_errors)
        sys.exit(65)
    if expression is not None:
        print(AstPrinter().print(expression))


def parse(content: str) -> tuple[list[stmt.Stmt[str]], list[str]]:
    """Parse the contents."""
    sc = scanner.Scanner(content)
    tokens = sc.scan_tokens()
    parser = Parser[str](tokens)
    statements = parser.parse()
    return statements, parser.parse_errors


def parse_expression(content: str) -> tuple[expr.Expr[str] | None, list[str]]:
    """Parse the contents."""
    sc = scanner.Scanner(content)
    tokens = sc.scan_tokens()
    parser = Parser[str](tokens)
    try:
        _expr = parser.expression()
    except Parser.ParseError:
        _expr = None
    return _expr, parser.parse_errors


def _print_parse_errors(errors: list[str]) -> None:
    """Print a list of parse errors"""
    for error in errors:
        print(error, file=sys.stderr)
